{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f83a91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cpfs/user/zhengziwei/ENV/miniconda3/envs/verl_agent/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/cpfs/user/zhengziwei/ENV/miniconda3/envs/verl_agent/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "from openai import OpenAI\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061c0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://10.39.11.28:10000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "response = requests.get(f\"{openai_api_base}/models\")\n",
    "models = response.json()\n",
    "model_name = models['data'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbba3c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vstar_bench_path = '/cpfs/user/honglingyi/DATA/LLM/Vstar/vstar_bench'\n",
    "test_types = ['direct_attributes', 'relative_position']\n",
    "per_type_acc = {}\n",
    "for test_type in test_types:\n",
    "    per_type_acc[test_type] = []\n",
    "all_acc = []\n",
    "abc_map = {1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea2c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_template():\n",
    "    chat_template = \"\"\"\n",
    "Below are two answers to a question. Question is [Question], [Standard Answer] is the standard answer to the question, and [Model_answer] is the answer extracted from a model's output to this question.  Determine whether these two answers are consistent.\n",
    "Note that [Model Answer] is consistent with [Standard Answer] whenever they are essentially the same. If the meaning is expressed in the same way, it is considered consistent, for example, 'pink' and 'it is pink'.\n",
    "If they are consistent, Judement is 1; if they are different, Judement is 0. Just output Judement and don't output anything else.\\n\\n\n",
    "\"\"\"\n",
    "    return chat_template\n",
    "\n",
    "def get_gpt4_score_ICE():\n",
    "    example_1 = \"\"\"\n",
    "[Question]: Is the countertop tan or blue?\n",
    "[Standard Answer]: A. The countertop is tan.\n",
    "[Model_answer] : tan\n",
    "Judgement: 1\n",
    "\"\"\" # noqa\n",
    "\n",
    "    example_2 = \"\"\"\n",
    "[Question]: On which side of the picture is the barrier?\n",
    "[Standard Answer]: A. The barrier is on the left side of the picture.\n",
    "[Model_answer] : A\n",
    "Judgement: 1\n",
    "\"\"\" # noqa\n",
    "\n",
    "    example_3 = \"\"\"\n",
    "[Question]: Is the kite brown and large?\n",
    "[Standard Answer]: A. Yes, the kite is brown and large.\n",
    "[Model_answer] : Yes\n",
    "Judgement: 1\n",
    "\"\"\" # noqa\n",
    "\n",
    "    example_4 = \"\"\"\n",
    "[Question]: Are the spots on a giraffe?\n",
    "[Standard Answer]: A. No, the spots are on a banana.\n",
    "[Model_answer] : no\n",
    "Judgement: 1\n",
    "\"\"\" # noqa\n",
    "\n",
    "    example_5 = \"\"\"\n",
    "[Question]: Who is wearing pants?\n",
    "[Standard Answer]: A. The boy is wearing pants.\n",
    "[Model_answer] : C. The girl in the picture is wearing pants.\n",
    "Judgement: 0\n",
    "\"\"\" # noqa\n",
    "\n",
    "    example_6 = \"\"\"\n",
    "[Question]: Is the man phone both blue and closed?\n",
    "[Standard Answer]: A. Yes, the man phone is both blue and closed.\n",
    "[Model_answer] : No.\n",
    "Judgement: 0\n",
    "\"\"\" # noqa\n",
    "\n",
    "    example_7 = \"\"\"\n",
    "[Question]: What color is the towel in the center of the picture?\n",
    "[Standard Answer]: A. The towel in the center of the picture is blue.\n",
    "[Model_answer] : The towel in the center of the picture is pink.\n",
    "Judgement: 0\n",
    "\"\"\" # noqa\n",
    "\n",
    "    return [example_1, example_2, example_3, example_4, example_5, example_6, example_7]\n",
    "\n",
    "def get_prompt(predict_str, ground_truth, question):\n",
    "    examples = get_gpt4_score_ICE()\n",
    "    chat_template = get_chat_template()\n",
    "    demo_prompt = chat_template\n",
    "    for example in examples:\n",
    "        demo_prompt += example + '\\n\\n'\n",
    "    test_prompt = f\"\"\"\n",
    "[Question]: {question}\n",
    "[Standard Answer]: {ground_truth}\n",
    "[Model_answer] : {predict_str}\n",
    "Judgement:\"\"\"\n",
    "    full_prompt = f'{demo_prompt}{test_prompt}'\n",
    "\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a81fbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for direct_attributes: 91.30%\n",
      "Accuracy for relative_position: 84.21%\n",
      "Overall Accuracy: 88.48%\n",
      "{'direct_attributes': {'tool_acc': 105.0, 'tool_wrong': 10.0, 'notool_acc': 0.0, 'notoool_wrong': 0.0}, 'relative_position': {'tool_acc': 64.0, 'tool_wrong': 12.0, 'notool_acc': 0.0, 'notoool_wrong': 0.0}}\n",
      "{'direct_attributes': 0, 'relative_position': 0}\n"
     ]
    }
   ],
   "source": [
    "result_root_path = '/cpfs/user/zhengziwei/workspace/agent/VeRL-Agent/eval_results/vlagent_think_mh/multiturn_all21k_0.8acc_-0.2format_0.4tool_4node_ds4_bugfree/global_step_48'\n",
    "all_acc = []\n",
    "per_type_acc = {}\n",
    "per_type_tool_acc = {}\n",
    "error_nums_type = {}\n",
    "tool_mode = ['tool_acc', 'tool_wrong', 'notool_acc', 'notoool_wrong']\n",
    "for test_type in test_types:\n",
    "    per_type_acc[test_type] = []\n",
    "    per_type_tool_acc[test_type] = {}\n",
    "    error_nums_type[test_type] = 0\n",
    "    for tm in tool_mode:\n",
    "        per_type_tool_acc[test_type][tm] = []\n",
    "for test_type in test_types:\n",
    "    result_path = os.path.join(result_root_path, \"result_\"+test_type+'.jsonl')\n",
    "    save_json = []\n",
    "    with open(result_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        data = json.loads(line)\n",
    "        question = data['question']\n",
    "        answer = data['answer']\n",
    "        pred_ans = data['pred_ans']\n",
    "        pred_output = data['pred_output']\n",
    "        answer = 'A. ' + answer\n",
    "\n",
    "        if '\\\\boxed' in pred_ans:\n",
    "            pred_ans = pred_ans.split('\\\\boxed{')[1].split('}')[0]\n",
    "\n",
    "        \n",
    "\n",
    "        # rule base check\n",
    "        if len(pred_ans)==1:\n",
    "            if pred_ans == 'A':\n",
    "                acc_reward = 1.0\n",
    "        elif len(pred_ans) == 2 and '.' in pred_ans:\n",
    "            if 'A' in pred_ans:\n",
    "                acc_reward = 1.0\n",
    "        elif answer in pred_ans:\n",
    "            acc_reward = 1.0\n",
    "        else:\n",
    "            full_prompt = get_prompt(pred_ans, answer, question)\n",
    "\n",
    "            chat_response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": full_prompt},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            response = chat_response.choices[0].message.content.strip()\n",
    "            if 'ERROR' in pred_ans:\n",
    "                error_nums_type[test_type] += 1\n",
    "            # print (pred_ans)\n",
    "            # print (answer)   \n",
    "            # print ('acc', response)\n",
    "\n",
    "\n",
    "            if 'Judgement:' in response:\n",
    "                response = response.split('Judgement:')[-1].strip()\n",
    "                if '1' in response:\n",
    "                    acc_reward = 1.0\n",
    "                elif '0' in response:\n",
    "                    acc_reward = 0.0\n",
    "                else:\n",
    "                    print(f' [WARNING] resp format error {response=}')\n",
    "                    acc_reward = 0.0\n",
    "            else:\n",
    "                if response == '1':\n",
    "                    acc_reward = 1.0\n",
    "                elif response == '0':\n",
    "                    acc_reward = 0.0\n",
    "                else:\n",
    "                    print(f' [WARNING] resp format error {response=}')\n",
    "                    acc_reward = 0.0\n",
    "\n",
    "        acc = acc_reward\n",
    "        all_acc.append(acc)\n",
    "        per_type_acc[test_type].append(acc)\n",
    "        \n",
    "        tool_use = False\n",
    "        for p_out in pred_output:\n",
    "            if p_out['role'] == 'system' or p_out['role'] == 'user':\n",
    "                continue\n",
    "            p_content = p_out['content']\n",
    "            if type(p_content) == str:\n",
    "                p_content_msg = p_content.strip()\n",
    "            elif type(p_content) == list:\n",
    "                for _p_content in p_content:\n",
    "                    if _p_content['type'] == 'text':\n",
    "                        p_content_msg = _p_content['text']\n",
    "            if '<tool_call>' in p_content_msg:\n",
    "                tool_use = True\n",
    "\n",
    "        if tool_use:\n",
    "            if acc == 1.0:\n",
    "                per_type_tool_acc[test_type]['tool_acc'].append(1.)\n",
    "            else:\n",
    "                per_type_tool_acc[test_type]['tool_wrong'].append(1.)\n",
    "        else:\n",
    "            if acc == 1.0:\n",
    "                per_type_tool_acc[test_type]['notool_acc'].append(1.)\n",
    "            else:\n",
    "                per_type_tool_acc[test_type]['notoool_wrong'].append(1.)\n",
    "\n",
    "        data['acc'] = acc\n",
    "        \n",
    "        save_json.append(data)\n",
    "    \n",
    "    with open(os.path.join(result_root_path, \"result_\"+test_type+'_tmp_acc.jsonl'), 'w') as f:\n",
    "        for item in save_json:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "            \n",
    "\n",
    "for test_type in test_types:\n",
    "    print(f\"Accuracy for {test_type}: {np.mean(per_type_acc[test_type]) * 100:.2f}%\")\n",
    "print(f\"Overall Accuracy: {np.mean(all_acc) * 100:.2f}%\")\n",
    "for test_type in test_types:\n",
    "    for tm in tool_mode:\n",
    "        per_type_tool_acc[test_type][tm] = np.sum(per_type_tool_acc[test_type][tm])\n",
    "print (per_type_tool_acc)\n",
    "print (error_nums_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3de1e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
